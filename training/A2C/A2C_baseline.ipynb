{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebooka327ed1c20",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-ganza007/Kuiper_Summative/blob/main/training/A2C/A2C_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gymnasium pygame numpy stable-baselines3[extra] tensorboard tqdm seaborn ipywidgets tensorflow\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "i42KQC0tRjGZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pygame\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "base_dir = \"/kaggle/working\" if 'kaggle' in str(os.uname()).lower() else \".\"\n",
        "os.makedirs(f\"{base_dir}/models/a2c\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/results/a2c\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/logs/a2c\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/tensorboard\", exist_ok=True)\n",
        "print(\"Setup complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T20:19:08.130207Z",
          "iopub.execute_input": "2025-11-20T20:19:08.130528Z",
          "iopub.status.idle": "2025-11-20T20:19:15.164028Z",
          "shell.execute_reply.started": "2025-11-20T20:19:08.130508Z",
          "shell.execute_reply": "2025-11-20T20:19:15.163303Z"
        },
        "id": "DqxrTF72RjGb",
        "outputId": "5be11d64-79c6-41a0-f4a4-6a0068de5b3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import resource_stream, resource_exists\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n2025-11-20 20:19:11.774662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763669951.806236     170 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763669951.813394     170 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Setup complete!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedMetricsCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.timesteps = []\n",
        "        self.successes = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if any(self.locals[\"dones\"]):\n",
        "            for i, done in enumerate(self.locals[\"dones\"]):\n",
        "                if done and \"episode\" in self.locals[\"infos\"][i]:\n",
        "                    info = self.locals[\"infos\"][i][\"episode\"]\n",
        "                    self.ep_rewards.append(info[\"r\"])\n",
        "                    self.ep_lengths.append(info[\"l\"])\n",
        "                    self.timesteps.append(self.num_timesteps)\n",
        "                    self.successes.append(info[\"r\"] >= 100)\n",
        "        return True\n",
        "\n",
        "class MissionEnvironment(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, grid_size=10, num_missions=3, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_missions = num_missions\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "\n",
        "        obs_size = 4 + (num_missions * 3)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0] + [0, -grid_size, -grid_size] * num_missions, dtype=np.float32),\n",
        "            high=np.array([grid_size, grid_size, 200, num_missions] + [np.sqrt(2*grid_size**2), grid_size, grid_size] * num_missions, dtype=np.float32),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.agent_pos = None\n",
        "        self.missions = None\n",
        "        self.completed_missions = None\n",
        "        self.fuel = 200\n",
        "        self.max_fuel = 200\n",
        "        self.steps = 0\n",
        "        self.max_steps = 500\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = np.array(\n",
        "            [self.np_random.integers(0, self.grid_size),\n",
        "             self.np_random.integers(0, self.grid_size)],\n",
        "            dtype=np.int32\n",
        "        )\n",
        "        self.missions = []\n",
        "        for _ in range(self.num_missions):\n",
        "            while True:\n",
        "                mission = np.array(\n",
        "                    [self.np_random.integers(0, self.grid_size),\n",
        "                     self.np_random.integers(0, self.grid_size)],\n",
        "                    dtype=np.int32\n",
        "                )\n",
        "                if not np.array_equal(mission, self.agent_pos) and \\\n",
        "                   not any(np.array_equal(mission, m) for m in self.missions):\n",
        "                    self.missions.append(mission)\n",
        "                    break\n",
        "        self.completed_missions = np.zeros(self.num_missions, dtype=bool)\n",
        "        self.fuel = self.max_fuel\n",
        "        self.steps = 0\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = np.zeros(4 + self.num_missions * 3, dtype=np.float32)\n",
        "        obs[0] = float(self.agent_pos[0])\n",
        "        obs[1] = float(self.agent_pos[1])\n",
        "        obs[2] = float(self.fuel)\n",
        "        obs[3] = float(np.sum(self.completed_missions))\n",
        "\n",
        "        mission_idx = 0\n",
        "        for i in range(self.num_missions):\n",
        "            if not self.completed_missions[i]:\n",
        "                dx = float(self.missions[i][0] - self.agent_pos[0])\n",
        "                dy = float(self.missions[i][1] - self.agent_pos[1])\n",
        "                dist = np.sqrt(dx**2 + dy**2)\n",
        "                obs[4 + mission_idx*3] = dist\n",
        "                obs[4 + mission_idx*3 + 1] = dx\n",
        "                obs[4 + mission_idx*3 + 2] = dy\n",
        "            else:\n",
        "                obs[4 + mission_idx*3] = 0.0\n",
        "                obs[4 + mission_idx*3 + 1] = 0.0\n",
        "                obs[4 + mission_idx*3 + 2] = 0.0\n",
        "            mission_idx += 1\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        reward = -0.1\n",
        "        terminated = False\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        if action < 4:\n",
        "            if self.fuel > 0:\n",
        "                deltas = [(-1, 0), (1, 0), (0, -1), (0, 1)][action]\n",
        "                new_pos = self.agent_pos + np.array(deltas)\n",
        "\n",
        "                if 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size:\n",
        "                    self.agent_pos = new_pos\n",
        "                    self.fuel -= 1\n",
        "\n",
        "                    active_mission_indices = [i for i in range(self.num_missions)\n",
        "                                            if not self.completed_missions[i]]\n",
        "                    if active_mission_indices:\n",
        "                        distances = [np.linalg.norm(self.agent_pos - self.missions[i])\n",
        "                                   for i in active_mission_indices]\n",
        "                        min_dist = np.min(distances)\n",
        "                        reward += 2.0 / (1.0 + min_dist)\n",
        "                else:\n",
        "                    reward -= 0.2\n",
        "            else:\n",
        "                reward -= 0.1\n",
        "\n",
        "        elif action == 4:\n",
        "            mission_completed = False\n",
        "            for i, mission in enumerate(self.missions):\n",
        "                if not self.completed_missions[i] and np.array_equal(self.agent_pos, mission):\n",
        "                    self.completed_missions[i] = True\n",
        "                    reward += 200.0\n",
        "                    mission_completed = True\n",
        "                    self.fuel = min(self.max_fuel, self.fuel + 30)\n",
        "                    break\n",
        "\n",
        "            if not mission_completed:\n",
        "                reward -= 0.5\n",
        "\n",
        "        completed_count = np.sum(self.completed_missions)\n",
        "        if completed_count == self.num_missions:\n",
        "            reward += 500.0\n",
        "            reward += self.fuel * 0.5\n",
        "            terminated = True\n",
        "\n",
        "        if self.fuel <= 0 and not terminated:\n",
        "            reward -= 10.0\n",
        "            terminated = True\n",
        "\n",
        "        if truncated and not terminated:\n",
        "            reward -= 5.0\n",
        "\n",
        "        return self._get_observation(), reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            return\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            import pygame\n",
        "            pygame.init()\n",
        "            self.window = pygame.display.set_mode((600, 600))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        import pygame\n",
        "        canvas = pygame.Surface((600, 600))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix = 600 / self.grid_size\n",
        "\n",
        "        for i, m in enumerate(self.missions):\n",
        "            color = (100, 200, 100) if self.completed_missions[i] else (200, 100, 100)\n",
        "            pygame.draw.rect(canvas, color, (m[0]*pix, m[1]*pix, pix, pix))\n",
        "\n",
        "        pygame.draw.circle(canvas, (0, 0, 255),\n",
        "                          ((self.agent_pos[0]+0.5)*pix, (self.agent_pos[1]+0.5)*pix),\n",
        "                          pix/3)\n",
        "\n",
        "        for x in range(self.grid_size + 1):\n",
        "            pygame.draw.line(canvas, (200, 200, 200), (x*pix, 0), (x*pix, 600))\n",
        "            pygame.draw.line(canvas, (200, 200, 200), (0, x*pix), (600, x*pix))\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.window.blit(canvas, (0, 0))\n",
        "            pygame.display.flip()\n",
        "            self.clock.tick(30)\n",
        "\n",
        "    def close(self):\n",
        "        if self.window:\n",
        "            import pygame\n",
        "            pygame.quit()\n",
        "            self.window = None\n",
        "\n",
        "gym.register(id=\"MissionEnv-v0\", entry_point=\"__main__:MissionEnvironment\",\n",
        "             max_episode_steps=500)\n",
        "print(\"MissionEnv-v0 registered!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:33:33.990879Z",
          "iopub.execute_input": "2025-11-20T21:33:33.991503Z",
          "iopub.status.idle": "2025-11-20T21:33:34.012944Z",
          "shell.execute_reply.started": "2025-11-20T21:33:33.99148Z",
          "shell.execute_reply": "2025-11-20T21:33:34.012355Z"
        },
        "id": "vV_9PK1LRjGd",
        "outputId": "5ae201d8-9961-4b9b-b8c9-d1c1e0482e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "MissionEnv-v0 registered!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "A2C_CONFIGS = {\n",
        "    \"baseline\": {\n",
        "        \"learning_rate\": 7e-4,\n",
        "        \"n_steps\": 5,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 1.0,\n",
        "        \"ent_coef\": 0.0,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": False\n",
        "    },\n",
        "\n",
        "    \"high_exploration\": {\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"n_steps\": 128,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"ent_coef\": 0.15,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.8,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"long_horizon\": {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"n_steps\": 512,\n",
        "        \"gamma\": 0.995,\n",
        "        \"gae_lambda\": 0.98,\n",
        "        \"ent_coef\": 0.02,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"conservative\": {\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"n_steps\": 64,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.9,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.8,\n",
        "        \"max_grad_norm\": 0.3,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"fast_learning\": {\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"n_steps\": 32,\n",
        "        \"gamma\": 0.95,\n",
        "        \"gae_lambda\": 0.9,\n",
        "        \"ent_coef\": 0.08,\n",
        "        \"vf_coef\": 0.3,\n",
        "        \"max_grad_norm\": 1.2,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": False\n",
        "    },\n",
        "\n",
        "    \"balanced\": {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"n_steps\": 256,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"ent_coef\": 0.06,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.8,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"value_focused\": {\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"n_steps\": 128,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.92,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 1.0,\n",
        "        \"max_grad_norm\": 0.6,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"policy_focused\": {\n",
        "        \"learning_rate\": 4e-4,\n",
        "        \"n_steps\": 192,\n",
        "        \"gamma\": 0.98,\n",
        "        \"gae_lambda\": 0.97,\n",
        "        \"ent_coef\": 0.12,\n",
        "        \"vf_coef\": 0.2,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": False\n",
        "    },\n",
        "\n",
        "    \"adaptive\": {\n",
        "        \"learning_rate\": 6e-4,\n",
        "        \"n_steps\": 384,\n",
        "        \"gamma\": 0.992,\n",
        "        \"gae_lambda\": 0.96,\n",
        "        \"ent_coef\": 0.04,\n",
        "        \"vf_coef\": 0.6,\n",
        "        \"max_grad_norm\": 0.7,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    },\n",
        "\n",
        "    \"aggressive\": {\n",
        "        \"learning_rate\": 8e-4,\n",
        "        \"n_steps\": 16,\n",
        "        \"gamma\": 0.9,\n",
        "        \"gae_lambda\": 0.8,\n",
        "        \"ent_coef\": 0.2,\n",
        "        \"vf_coef\": 0.1,\n",
        "        \"max_grad_norm\": 2.0,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": False\n",
        "    },\n",
        "\n",
        "    \"improved_v2\": {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"n_steps\": 256,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"ent_coef\": 0.1,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.8,\n",
        "        \"rms_prop_eps\": 1e-5,\n",
        "        \"use_rms_prop\": True,\n",
        "        \"normalize_advantage\": True\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:50:08.45211Z",
          "iopub.execute_input": "2025-11-20T21:50:08.452951Z",
          "iopub.status.idle": "2025-11-20T21:50:08.462033Z",
          "shell.execute_reply.started": "2025-11-20T21:50:08.452922Z",
          "shell.execute_reply": "2025-11-20T21:50:08.461274Z"
        },
        "id": "pbfZHWl6RjGg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "def train_a2c(config_name: str, total_timesteps=500_000, n_envs=8):\n",
        "    config = A2C_CONFIGS[config_name]\n",
        "    print(f\"\\n{'='*80}\\nTRAINING A2C → {config_name.upper()}\\n{'='*80}\")\n",
        "\n",
        "    env = make_vec_env(\"MissionEnv-v0\", n_envs=n_envs, wrapper_class=Monitor)\n",
        "    eval_env = Monitor(gym.make(\"MissionEnv-v0\"))\n",
        "\n",
        "    metrics_cb = AdvancedMetricsCallback()\n",
        "    eval_cb = EvalCallback(eval_env, eval_freq=10000, n_eval_episodes=20, deterministic=True, verbose=0)\n",
        "\n",
        "    model = A2C(\"MlpPolicy\", env,\n",
        "                policy_kwargs=dict(\n",
        "                    net_arch=[256, 256],\n",
        "                    activation_fn=torch.nn.ReLU\n",
        "                ),\n",
        "                tensorboard_log=f\"{base_dir}/tensorboard\",\n",
        "                verbose=1,\n",
        "                **config)\n",
        "\n",
        "    start = time.time()\n",
        "    model.learn(total_timesteps=total_timesteps, callback=[eval_cb, metrics_cb], progress_bar=True)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    rewards = np.array(metrics_cb.ep_rewards)\n",
        "    if len(rewards) > 50:\n",
        "        rolling = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
        "        threshold = np.percentile(rewards, 90)\n",
        "        conv_idx = next((i for i, r in enumerate(rolling) if np.all(rolling[i:i+20] >= threshold)), None)\n",
        "        convergence_ts = metrics_cb.timesteps[conv_idx + 19] if conv_idx is not None else None\n",
        "    else:\n",
        "        convergence_ts = None\n",
        "\n",
        "    model_path = f\"{base_dir}/models/a2c/a2c_{config_name}.zip\"\n",
        "    model.save(model_path)\n",
        "    print(f\"✓ MODEL SAVED: {model_path}\")\n",
        "\n",
        "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50, deterministic=True)\n",
        "    success_rate = np.mean(metrics_cb.successes[-100:]) * 100 if metrics_cb.successes else 0\n",
        "\n",
        "    results = {\n",
        "        \"config\": config_name,\n",
        "        \"mean_reward\": round(float(mean_reward), 2),\n",
        "        \"std_reward\": round(float(std_reward), 2),\n",
        "        \"avg_steps_per_episode\": round(np.mean(metrics_cb.ep_lengths[-50:]), 1),\n",
        "        \"success_rate_%\": round(success_rate, 1),\n",
        "        \"convergence_timestep\": int(convergence_ts) if convergence_ts else None,\n",
        "        \"train_time_min\": round(train_time/60, 2),\n",
        "        \"total_timesteps\": total_timesteps,\n",
        "        \"model_path\": model_path\n",
        "    }\n",
        "\n",
        "    with open(f\"{base_dir}/results/a2c/{config_name}_results.json\", \"w\") as f:\n",
        "        json.dump({**results, \"rewards_history\": metrics_cb.ep_rewards[-500:],\n",
        "                   \"lengths_history\": metrics_cb.ep_lengths[-500:]}, f, indent=4)\n",
        "\n",
        "    print(f\"Reward: {mean_reward:+.1f} ± {std_reward:.1f} | Steps/Episode: {results['avg_steps_per_episode']}\")\n",
        "    print(f\"Success: {success_rate:.1f}% | Convergence: {convergence_ts or 'N/A'} steps\")\n",
        "    env.close(); eval_env.close()\n",
        "    return results, metrics_cb\n",
        "\n",
        "train_a2c(\"baseline\", total_timesteps=500_000, n_envs=8)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:50:14.054694Z",
          "iopub.execute_input": "2025-11-20T21:50:14.05499Z",
          "iopub.status.idle": "2025-11-20T21:53:53.299671Z",
          "shell.execute_reply.started": "2025-11-20T21:50:14.054968Z",
          "shell.execute_reply": "2025-11-20T21:53:53.299052Z"
        },
        "id": "jYKf2PI9RjGi",
        "outputId": "bc3c4dc4-d0e5-4185-9104-5cf5593bfc82",
        "colab": {
          "referenced_widgets": [
            "1119b9c1f34242b3861a88c6d2c50916"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n================================================================================\nTRAINING A2C → BASELINE\n================================================================================\nUsing cuda device\nLogging to ./tensorboard/A2C_7\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1119b9c1f34242b3861a88c6d2c50916"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -145      |\n| time/                 |           |\n|    fps                | 3260      |\n|    iterations         | 100       |\n|    time_elapsed       | 1         |\n|    total_timesteps    | 4000      |\n| train/                |           |\n|    entropy_loss       | -2.68e-05 |\n|    explained_variance | 0.993     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 99        |\n|    policy_loss        | 3.61e-08  |\n|    value_loss         | 0.000625  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -148      |\n| time/                 |           |\n|    fps                | 3283      |\n|    iterations         | 200       |\n|    time_elapsed       | 2         |\n|    total_timesteps    | 8000      |\n| train/                |           |\n|    entropy_loss       | -1.83e-05 |\n|    explained_variance | 0.99      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 199       |\n|    policy_loss        | 1.04e-07  |\n|    value_loss         | 0.00771   |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -149     |\n| time/                 |          |\n|    fps                | 3294     |\n|    iterations         | 300      |\n|    time_elapsed       | 3        |\n|    total_timesteps    | 12000    |\n| train/                |          |\n|    entropy_loss       | -1.8e-05 |\n|    explained_variance | 0.973    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 299      |\n|    policy_loss        | 8.88e-08 |\n|    value_loss         | 0.00506  |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -149      |\n| time/                 |           |\n|    fps                | 3296      |\n|    iterations         | 400       |\n|    time_elapsed       | 4         |\n|    total_timesteps    | 16000     |\n| train/                |           |\n|    entropy_loss       | -2.17e-05 |\n|    explained_variance | 0.994     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 399       |\n|    policy_loss        | 1.01e-07  |\n|    value_loss         | 0.00516   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -150      |\n| time/                 |           |\n|    fps                | 3298      |\n|    iterations         | 500       |\n|    time_elapsed       | 6         |\n|    total_timesteps    | 20000     |\n| train/                |           |\n|    entropy_loss       | -2.26e-05 |\n|    explained_variance | 0.993     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 499       |\n|    policy_loss        | 8.45e-08  |\n|    value_loss         | 0.00247   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -150      |\n| time/                 |           |\n|    fps                | 3298      |\n|    iterations         | 600       |\n|    time_elapsed       | 7         |\n|    total_timesteps    | 24000     |\n| train/                |           |\n|    entropy_loss       | -2.27e-05 |\n|    explained_variance | 0.993     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 599       |\n|    policy_loss        | 9.55e-08  |\n|    value_loss         | 0.00315   |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -150     |\n| time/                 |          |\n|    fps                | 3284     |\n|    iterations         | 700      |\n|    time_elapsed       | 8        |\n|    total_timesteps    | 28000    |\n| train/                |          |\n|    entropy_loss       | -2.5e-05 |\n|    explained_variance | 0.99     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 699      |\n|    policy_loss        | 5.78e-08 |\n|    value_loss         | 0.00114  |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -150      |\n| time/                 |           |\n|    fps                | 3268      |\n|    iterations         | 800       |\n|    time_elapsed       | 9         |\n|    total_timesteps    | 32000     |\n| train/                |           |\n|    entropy_loss       | -2.37e-05 |\n|    explained_variance | 0.989     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 799       |\n|    policy_loss        | 8.64e-08  |\n|    value_loss         | 0.00255   |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 3268     |\n|    iterations         | 900      |\n|    time_elapsed       | 11       |\n|    total_timesteps    | 36000    |\n| train/                |          |\n|    entropy_loss       | -3e-05   |\n|    explained_variance | 0.989    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 899      |\n|    policy_loss        | 8.72e-08 |\n|    value_loss         | 0.0026   |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 3268      |\n|    iterations         | 1000      |\n|    time_elapsed       | 12        |\n|    total_timesteps    | 40000     |\n| train/                |           |\n|    entropy_loss       | -3.04e-05 |\n|    explained_variance | 0.88      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 999       |\n|    policy_loss        | 7.85e-08  |\n|    value_loss         | 0.00206   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 3270      |\n|    iterations         | 1100      |\n|    time_elapsed       | 13        |\n|    total_timesteps    | 44000     |\n| train/                |           |\n|    entropy_loss       | -2.93e-05 |\n|    explained_variance | 0.99      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1099      |\n|    policy_loss        | 7.29e-08  |\n|    value_loss         | 0.00182   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 3270      |\n|    iterations         | 1200      |\n|    time_elapsed       | 14        |\n|    total_timesteps    | 48000     |\n| train/                |           |\n|    entropy_loss       | -3.37e-05 |\n|    explained_variance | 0.987     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1199      |\n|    policy_loss        | 8.33e-08  |\n|    value_loss         | 0.00195   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 3265      |\n|    iterations         | 1300      |\n|    time_elapsed       | 15        |\n|    total_timesteps    | 52000     |\n| train/                |           |\n|    entropy_loss       | -7.03e-05 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1299      |\n|    policy_loss        | 1.84e-07  |\n|    value_loss         | 0.00168   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 3269      |\n|    iterations         | 1400      |\n|    time_elapsed       | 17        |\n|    total_timesteps    | 56000     |\n| train/                |           |\n|    entropy_loss       | -0.000105 |\n|    explained_variance | 0.987     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1399      |\n|    policy_loss        | 3.86e-07  |\n|    value_loss         | 0.00286   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 3261      |\n|    iterations         | 1500      |\n|    time_elapsed       | 18        |\n|    total_timesteps    | 60000     |\n| train/                |           |\n|    entropy_loss       | -0.000117 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1499      |\n|    policy_loss        | 1.48e-07  |\n|    value_loss         | 0.000406  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 3262      |\n|    iterations         | 1600      |\n|    time_elapsed       | 19        |\n|    total_timesteps    | 64000     |\n| train/                |           |\n|    entropy_loss       | -8.97e-05 |\n|    explained_variance | 0.994     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1599      |\n|    policy_loss        | 1.57e-07  |\n|    value_loss         | 0.00072   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 3261      |\n|    iterations         | 1700      |\n|    time_elapsed       | 20        |\n|    total_timesteps    | 68000     |\n| train/                |           |\n|    entropy_loss       | -0.000127 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1699      |\n|    policy_loss        | 1.72e-07  |\n|    value_loss         | 0.000471  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 3261      |\n|    iterations         | 1800      |\n|    time_elapsed       | 22        |\n|    total_timesteps    | 72000     |\n| train/                |           |\n|    entropy_loss       | -0.000183 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1799      |\n|    policy_loss        | 3.43e-07  |\n|    value_loss         | 0.000861  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 3261     |\n|    iterations         | 1900     |\n|    time_elapsed       | 23       |\n|    total_timesteps    | 76000    |\n| train/                |          |\n|    entropy_loss       | -0.00372 |\n|    explained_variance | 0.999    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1899     |\n|    policy_loss        | 1.3e-06  |\n|    value_loss         | 8.01e-05 |\n------------------------------------\n-------------------------------------\n| eval/                 |           |\n|    mean_ep_length     | 500       |\n|    mean_reward        | -151      |\n| time/                 |           |\n|    total_timesteps    | 80000     |\n| train/                |           |\n|    entropy_loss       | -0.00323  |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1999      |\n|    policy_loss        | -2.34e-06 |\n|    value_loss         | 6.31e-05  |\n-------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 500      |\n|    ep_rew_mean     | -151     |\n| time/              |          |\n|    fps             | 2456     |\n|    iterations      | 2000     |\n|    time_elapsed    | 32       |\n|    total_timesteps | 80000    |\n---------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2486      |\n|    iterations         | 2100      |\n|    time_elapsed       | 33        |\n|    total_timesteps    | 84000     |\n| train/                |           |\n|    entropy_loss       | -0.00232  |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2099      |\n|    policy_loss        | -4.92e-06 |\n|    value_loss         | 0.00053   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2513      |\n|    iterations         | 2200      |\n|    time_elapsed       | 35        |\n|    total_timesteps    | 88000     |\n| train/                |           |\n|    entropy_loss       | -3.98e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2199      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000189  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2539     |\n|    iterations         | 2300     |\n|    time_elapsed       | 36       |\n|    total_timesteps    | 92000    |\n| train/                |          |\n|    entropy_loss       | -3.9e-06 |\n|    explained_variance | 0.994    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2299     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000447 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2563      |\n|    iterations         | 2400      |\n|    time_elapsed       | 37        |\n|    total_timesteps    | 96000     |\n| train/                |           |\n|    entropy_loss       | -3.71e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2399      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000164  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2584      |\n|    iterations         | 2500      |\n|    time_elapsed       | 38        |\n|    total_timesteps    | 100000    |\n| train/                |           |\n|    entropy_loss       | -4.06e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2499      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000184  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2605     |\n|    iterations         | 2600     |\n|    time_elapsed       | 39       |\n|    total_timesteps    | 104000   |\n| train/                |          |\n|    entropy_loss       | -3.5e-06 |\n|    explained_variance | 0.998    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2599     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000241 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2625      |\n|    iterations         | 2700      |\n|    time_elapsed       | 41        |\n|    total_timesteps    | 108000    |\n| train/                |           |\n|    entropy_loss       | -3.85e-06 |\n|    explained_variance | 0.992     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2699      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.0012    |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2643     |\n|    iterations         | 2800     |\n|    time_elapsed       | 42       |\n|    total_timesteps    | 112000   |\n| train/                |          |\n|    entropy_loss       | -4e-06   |\n|    explained_variance | 0.997    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2799     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.00015  |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2660      |\n|    iterations         | 2900      |\n|    time_elapsed       | 43        |\n|    total_timesteps    | 116000    |\n| train/                |           |\n|    entropy_loss       | -4.14e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2899      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000247  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2676      |\n|    iterations         | 3000      |\n|    time_elapsed       | 44        |\n|    total_timesteps    | 120000    |\n| train/                |           |\n|    entropy_loss       | -4.49e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2999      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000395  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2691      |\n|    iterations         | 3100      |\n|    time_elapsed       | 46        |\n|    total_timesteps    | 124000    |\n| train/                |           |\n|    entropy_loss       | -4.52e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3099      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00025   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2706      |\n|    iterations         | 3200      |\n|    time_elapsed       | 47        |\n|    total_timesteps    | 128000    |\n| train/                |           |\n|    entropy_loss       | -4.21e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3199      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000468  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2718      |\n|    iterations         | 3300      |\n|    time_elapsed       | 48        |\n|    total_timesteps    | 132000    |\n| train/                |           |\n|    entropy_loss       | -3.75e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3299      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000304  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2731      |\n|    iterations         | 3400      |\n|    time_elapsed       | 49        |\n|    total_timesteps    | 136000    |\n| train/                |           |\n|    entropy_loss       | -4.06e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3399      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000568  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2741      |\n|    iterations         | 3500      |\n|    time_elapsed       | 51        |\n|    total_timesteps    | 140000    |\n| train/                |           |\n|    entropy_loss       | -4.57e-06 |\n|    explained_variance | 0.99      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3499      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000397  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2753      |\n|    iterations         | 3600      |\n|    time_elapsed       | 52        |\n|    total_timesteps    | 144000    |\n| train/                |           |\n|    entropy_loss       | -4.23e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3599      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000603  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2765      |\n|    iterations         | 3700      |\n|    time_elapsed       | 53        |\n|    total_timesteps    | 148000    |\n| train/                |           |\n|    entropy_loss       | -4.96e-06 |\n|    explained_variance | 0.995     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3699      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000119  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2777      |\n|    iterations         | 3800      |\n|    time_elapsed       | 54        |\n|    total_timesteps    | 152000    |\n| train/                |           |\n|    entropy_loss       | -4.41e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3799      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000244  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2786      |\n|    iterations         | 3900      |\n|    time_elapsed       | 55        |\n|    total_timesteps    | 156000    |\n| train/                |           |\n|    entropy_loss       | -4.28e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3899      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000194  |\n-------------------------------------\n-------------------------------------\n| eval/                 |           |\n|    mean_ep_length     | 500       |\n|    mean_reward        | -150      |\n| time/                 |           |\n|    total_timesteps    | 160000    |\n| train/                |           |\n|    entropy_loss       | -4.83e-06 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 3999      |\n|    policy_loss        | -0        |\n|    value_loss         | 7.22e-05  |\n-------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 500      |\n|    ep_rew_mean     | -151     |\n| time/              |          |\n|    fps             | 2452     |\n|    iterations      | 4000     |\n|    time_elapsed    | 65       |\n|    total_timesteps | 160000   |\n---------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2467      |\n|    iterations         | 4100      |\n|    time_elapsed       | 66        |\n|    total_timesteps    | 164000    |\n| train/                |           |\n|    entropy_loss       | -4.65e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4099      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000303  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2481      |\n|    iterations         | 4200      |\n|    time_elapsed       | 67        |\n|    total_timesteps    | 168000    |\n| train/                |           |\n|    entropy_loss       | -4.71e-06 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4199      |\n|    policy_loss        | -0        |\n|    value_loss         | 4.12e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2495      |\n|    iterations         | 4300      |\n|    time_elapsed       | 68        |\n|    total_timesteps    | 172000    |\n| train/                |           |\n|    entropy_loss       | -4.77e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4299      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000207  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2508      |\n|    iterations         | 4400      |\n|    time_elapsed       | 70        |\n|    total_timesteps    | 176000    |\n| train/                |           |\n|    entropy_loss       | -5.51e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4399      |\n|    policy_loss        | -0        |\n|    value_loss         | 3.63e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2521      |\n|    iterations         | 4500      |\n|    time_elapsed       | 71        |\n|    total_timesteps    | 180000    |\n| train/                |           |\n|    entropy_loss       | -4.87e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4499      |\n|    policy_loss        | -0        |\n|    value_loss         | 9.38e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2534      |\n|    iterations         | 4600      |\n|    time_elapsed       | 72        |\n|    total_timesteps    | 184000    |\n| train/                |           |\n|    entropy_loss       | -5.29e-06 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4599      |\n|    policy_loss        | -0        |\n|    value_loss         | 3.22e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2547      |\n|    iterations         | 4700      |\n|    time_elapsed       | 73        |\n|    total_timesteps    | 188000    |\n| train/                |           |\n|    entropy_loss       | -4.91e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4699      |\n|    policy_loss        | -0        |\n|    value_loss         | 2.84e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2559      |\n|    iterations         | 4800      |\n|    time_elapsed       | 75        |\n|    total_timesteps    | 192000    |\n| train/                |           |\n|    entropy_loss       | -4.81e-06 |\n|    explained_variance | 0.994     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4799      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000473  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2571      |\n|    iterations         | 4900      |\n|    time_elapsed       | 76        |\n|    total_timesteps    | 196000    |\n| train/                |           |\n|    entropy_loss       | -5.52e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4899      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000147  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2582      |\n|    iterations         | 5000      |\n|    time_elapsed       | 77        |\n|    total_timesteps    | 200000    |\n| train/                |           |\n|    entropy_loss       | -4.79e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 4999      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000604  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2591      |\n|    iterations         | 5100      |\n|    time_elapsed       | 78        |\n|    total_timesteps    | 204000    |\n| train/                |           |\n|    entropy_loss       | -5.94e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5099      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000128  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2602      |\n|    iterations         | 5200      |\n|    time_elapsed       | 79        |\n|    total_timesteps    | 208000    |\n| train/                |           |\n|    entropy_loss       | -5.82e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5199      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000469  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2611      |\n|    iterations         | 5300      |\n|    time_elapsed       | 81        |\n|    total_timesteps    | 212000    |\n| train/                |           |\n|    entropy_loss       | -4.94e-06 |\n|    explained_variance | 0.988     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5299      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000537  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2621     |\n|    iterations         | 5400     |\n|    time_elapsed       | 82       |\n|    total_timesteps    | 216000   |\n| train/                |          |\n|    entropy_loss       | -4.7e-06 |\n|    explained_variance | 0.993    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5399     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000487 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2630      |\n|    iterations         | 5500      |\n|    time_elapsed       | 83        |\n|    total_timesteps    | 220000    |\n| train/                |           |\n|    entropy_loss       | -4.67e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5499      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000544  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2640      |\n|    iterations         | 5600      |\n|    time_elapsed       | 84        |\n|    total_timesteps    | 224000    |\n| train/                |           |\n|    entropy_loss       | -6.89e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5599      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000114  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2649      |\n|    iterations         | 5700      |\n|    time_elapsed       | 86        |\n|    total_timesteps    | 228000    |\n| train/                |           |\n|    entropy_loss       | -5.87e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5699      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000331  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2658      |\n|    iterations         | 5800      |\n|    time_elapsed       | 87        |\n|    total_timesteps    | 232000    |\n| train/                |           |\n|    entropy_loss       | -6.49e-06 |\n|    explained_variance | 0.993     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5799      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000456  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2665      |\n|    iterations         | 5900      |\n|    time_elapsed       | 88        |\n|    total_timesteps    | 236000    |\n| train/                |           |\n|    entropy_loss       | -6.25e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5899      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000375  |\n-------------------------------------\n-------------------------------------\n| eval/                 |           |\n|    mean_ep_length     | 500       |\n|    mean_reward        | -151      |\n| time/                 |           |\n|    total_timesteps    | 240000    |\n| train/                |           |\n|    entropy_loss       | -6.98e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 5999      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000692  |\n-------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 500      |\n|    ep_rew_mean     | -151     |\n| time/              |          |\n|    fps             | 2454     |\n|    iterations      | 6000     |\n|    time_elapsed    | 97       |\n|    total_timesteps | 240000   |\n---------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2463      |\n|    iterations         | 6100      |\n|    time_elapsed       | 99        |\n|    total_timesteps    | 244000    |\n| train/                |           |\n|    entropy_loss       | -6.45e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6099      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000408  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2473      |\n|    iterations         | 6200      |\n|    time_elapsed       | 100       |\n|    total_timesteps    | 248000    |\n| train/                |           |\n|    entropy_loss       | -6.69e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6199      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000266  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2483      |\n|    iterations         | 6300      |\n|    time_elapsed       | 101       |\n|    total_timesteps    | 252000    |\n| train/                |           |\n|    entropy_loss       | -8.24e-06 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6299      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000181  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2492      |\n|    iterations         | 6400      |\n|    time_elapsed       | 102       |\n|    total_timesteps    | 256000    |\n| train/                |           |\n|    entropy_loss       | -8.12e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6399      |\n|    policy_loss        | -0        |\n|    value_loss         | 2.68e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2501      |\n|    iterations         | 6500      |\n|    time_elapsed       | 103       |\n|    total_timesteps    | 260000    |\n| train/                |           |\n|    entropy_loss       | -7.84e-06 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6499      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000264  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2510      |\n|    iterations         | 6600      |\n|    time_elapsed       | 105       |\n|    total_timesteps    | 264000    |\n| train/                |           |\n|    entropy_loss       | -7.99e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6599      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000436  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2519      |\n|    iterations         | 6700      |\n|    time_elapsed       | 106       |\n|    total_timesteps    | 268000    |\n| train/                |           |\n|    entropy_loss       | -7.26e-06 |\n|    explained_variance | 0.961     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6699      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000609  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2528     |\n|    iterations         | 6800     |\n|    time_elapsed       | 107      |\n|    total_timesteps    | 272000   |\n| train/                |          |\n|    entropy_loss       | -8.7e-06 |\n|    explained_variance | 0.997    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6799     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000172 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2536      |\n|    iterations         | 6900      |\n|    time_elapsed       | 108       |\n|    total_timesteps    | 276000    |\n| train/                |           |\n|    entropy_loss       | -7.47e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6899      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000391  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2544      |\n|    iterations         | 7000      |\n|    time_elapsed       | 110       |\n|    total_timesteps    | 280000    |\n| train/                |           |\n|    entropy_loss       | -8.77e-06 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 6999      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000472  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2552      |\n|    iterations         | 7100      |\n|    time_elapsed       | 111       |\n|    total_timesteps    | 284000    |\n| train/                |           |\n|    entropy_loss       | -8.58e-06 |\n|    explained_variance | 0.995     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7099      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000473  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2560      |\n|    iterations         | 7200      |\n|    time_elapsed       | 112       |\n|    total_timesteps    | 288000    |\n| train/                |           |\n|    entropy_loss       | -9.09e-06 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7199      |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000291  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2567      |\n|    iterations         | 7300      |\n|    time_elapsed       | 113       |\n|    total_timesteps    | 292000    |\n| train/                |           |\n|    entropy_loss       | -9.74e-06 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7299      |\n|    policy_loss        | -0        |\n|    value_loss         | 4.62e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2575      |\n|    iterations         | 7400      |\n|    time_elapsed       | 114       |\n|    total_timesteps    | 296000    |\n| train/                |           |\n|    entropy_loss       | -9.71e-06 |\n|    explained_variance | 0.995     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7399      |\n|    policy_loss        | -0        |\n|    value_loss         | 7.36e-05  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2582     |\n|    iterations         | 7500     |\n|    time_elapsed       | 116      |\n|    total_timesteps    | 300000   |\n| train/                |          |\n|    entropy_loss       | -9.9e-06 |\n|    explained_variance | 0.996    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7499     |\n|    policy_loss        | -0       |\n|    value_loss         | 7.42e-05 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2589      |\n|    iterations         | 7600      |\n|    time_elapsed       | 117       |\n|    total_timesteps    | 304000    |\n| train/                |           |\n|    entropy_loss       | -1.31e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7599      |\n|    policy_loss        | -1.43e-08 |\n|    value_loss         | 0.000287  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2596     |\n|    iterations         | 7700     |\n|    time_elapsed       | 118      |\n|    total_timesteps    | 308000   |\n| train/                |          |\n|    entropy_loss       | -9.8e-06 |\n|    explained_variance | 0.993    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7699     |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000189 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2603      |\n|    iterations         | 7800      |\n|    time_elapsed       | 119       |\n|    total_timesteps    | 312000    |\n| train/                |           |\n|    entropy_loss       | -1.63e-05 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7799      |\n|    policy_loss        | 1.07e-08  |\n|    value_loss         | 0.000511  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2610      |\n|    iterations         | 7900      |\n|    time_elapsed       | 121       |\n|    total_timesteps    | 316000    |\n| train/                |           |\n|    entropy_loss       | -1.91e-05 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7899      |\n|    policy_loss        | 1.85e-08  |\n|    value_loss         | 0.000236  |\n-------------------------------------\n-------------------------------------\n| eval/                 |           |\n|    mean_ep_length     | 500       |\n|    mean_reward        | -152      |\n| time/                 |           |\n|    total_timesteps    | 320000    |\n| train/                |           |\n|    entropy_loss       | -2.25e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 7999      |\n|    policy_loss        | 3.47e-08  |\n|    value_loss         | 0.000465  |\n-------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 500      |\n|    ep_rew_mean     | -151     |\n| time/              |          |\n|    fps             | 2456     |\n|    iterations      | 8000     |\n|    time_elapsed    | 130      |\n|    total_timesteps | 320000   |\n---------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2463      |\n|    iterations         | 8100      |\n|    time_elapsed       | 131       |\n|    total_timesteps    | 324000    |\n| train/                |           |\n|    entropy_loss       | -2.28e-05 |\n|    explained_variance | 0.996     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8099      |\n|    policy_loss        | 4.19e-08  |\n|    value_loss         | 0.000637  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2471     |\n|    iterations         | 8200     |\n|    time_elapsed       | 132      |\n|    total_timesteps    | 328000   |\n| train/                |          |\n|    entropy_loss       | -2.7e-05 |\n|    explained_variance | 0.998    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8199     |\n|    policy_loss        | 2.3e-08  |\n|    value_loss         | 0.000236 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2478      |\n|    iterations         | 8300      |\n|    time_elapsed       | 133       |\n|    total_timesteps    | 332000    |\n| train/                |           |\n|    entropy_loss       | -2.77e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8299      |\n|    policy_loss        | 1.9e-08   |\n|    value_loss         | 0.000139  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2485      |\n|    iterations         | 8400      |\n|    time_elapsed       | 135       |\n|    total_timesteps    | 336000    |\n| train/                |           |\n|    entropy_loss       | -2.86e-05 |\n|    explained_variance | 0.99      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8399      |\n|    policy_loss        | 2.5e-08   |\n|    value_loss         | 0.000214  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2492      |\n|    iterations         | 8500      |\n|    time_elapsed       | 136       |\n|    total_timesteps    | 340000    |\n| train/                |           |\n|    entropy_loss       | -2.48e-05 |\n|    explained_variance | 0.954     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8499      |\n|    policy_loss        | 3.6e-08   |\n|    value_loss         | 0.000437  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2499      |\n|    iterations         | 8600      |\n|    time_elapsed       | 137       |\n|    total_timesteps    | 344000    |\n| train/                |           |\n|    entropy_loss       | -2.76e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8599      |\n|    policy_loss        | 2.47e-08  |\n|    value_loss         | 0.000263  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2505      |\n|    iterations         | 8700      |\n|    time_elapsed       | 138       |\n|    total_timesteps    | 348000    |\n| train/                |           |\n|    entropy_loss       | -3.39e-05 |\n|    explained_variance | 0.991     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8699      |\n|    policy_loss        | 3.49e-08  |\n|    value_loss         | 0.000338  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2512      |\n|    iterations         | 8800      |\n|    time_elapsed       | 140       |\n|    total_timesteps    | 352000    |\n| train/                |           |\n|    entropy_loss       | -3.63e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8799      |\n|    policy_loss        | 2.21e-08  |\n|    value_loss         | 0.000139  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2519      |\n|    iterations         | 8900      |\n|    time_elapsed       | 141       |\n|    total_timesteps    | 356000    |\n| train/                |           |\n|    entropy_loss       | -4.58e-05 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 8899      |\n|    policy_loss        | 5.15e-08  |\n|    value_loss         | 0.000352  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -151     |\n| time/                 |          |\n|    fps                | 2525     |\n|    iterations         | 9000     |\n|    time_elapsed       | 142      |\n|    total_timesteps    | 360000   |\n| train/                |          |\n|    entropy_loss       | -5.2e-05 |\n|    explained_variance | 0.98     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8999     |\n|    policy_loss        | 2.76e-08 |\n|    value_loss         | 7.38e-05 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2532      |\n|    iterations         | 9100      |\n|    time_elapsed       | 143       |\n|    total_timesteps    | 364000    |\n| train/                |           |\n|    entropy_loss       | -5.55e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 9099      |\n|    policy_loss        | 3.61e-08  |\n|    value_loss         | 0.000134  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2538      |\n|    iterations         | 9200      |\n|    time_elapsed       | 144       |\n|    total_timesteps    | 368000    |\n| train/                |           |\n|    entropy_loss       | -0.000126 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 9199      |\n|    policy_loss        | 1.93e-07  |\n|    value_loss         | 0.000605  |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 500      |\n|    ep_rew_mean        | -146     |\n| time/                 |          |\n|    fps                | 2543     |\n|    iterations         | 9300     |\n|    time_elapsed       | 146      |\n|    total_timesteps    | 372000   |\n| train/                |          |\n|    entropy_loss       | -0.994   |\n|    explained_variance | -0.202   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9299     |\n|    policy_loss        | 0.319    |\n|    value_loss         | 2.9      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 481      |\n|    ep_rew_mean        | -128     |\n| time/                 |          |\n|    fps                | 2547     |\n|    iterations         | 9400     |\n|    time_elapsed       | 147      |\n|    total_timesteps    | 376000   |\n| train/                |          |\n|    entropy_loss       | -1.27    |\n|    explained_variance | 0.829    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9399     |\n|    policy_loss        | 1.24     |\n|    value_loss         | 2.02     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 446      |\n|    ep_rew_mean        | -90.2    |\n| time/                 |          |\n|    fps                | 2550     |\n|    iterations         | 9500     |\n|    time_elapsed       | 149      |\n|    total_timesteps    | 380000   |\n| train/                |          |\n|    entropy_loss       | -0.861   |\n|    explained_variance | 0.916    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9499     |\n|    policy_loss        | 0.041    |\n|    value_loss         | 0.843    |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 408      |\n|    ep_rew_mean        | -49.7    |\n| time/                 |          |\n|    fps                | 2553     |\n|    iterations         | 9600     |\n|    time_elapsed       | 150      |\n|    total_timesteps    | 384000   |\n| train/                |          |\n|    entropy_loss       | -0.859   |\n|    explained_variance | 0.994    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9599     |\n|    policy_loss        | -0.818   |\n|    value_loss         | 2.08     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 354      |\n|    ep_rew_mean        | 2.09     |\n| time/                 |          |\n|    fps                | 2556     |\n|    iterations         | 9700     |\n|    time_elapsed       | 151      |\n|    total_timesteps    | 388000   |\n| train/                |          |\n|    entropy_loss       | -0.693   |\n|    explained_variance | 0.987    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9699     |\n|    policy_loss        | 0.863    |\n|    value_loss         | 14.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 320      |\n|    ep_rew_mean        | 35.1     |\n| time/                 |          |\n|    fps                | 2560     |\n|    iterations         | 9800     |\n|    time_elapsed       | 153      |\n|    total_timesteps    | 392000   |\n| train/                |          |\n|    entropy_loss       | -0.553   |\n|    explained_variance | 0.992    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9799     |\n|    policy_loss        | 0.441    |\n|    value_loss         | 0.949    |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 263      |\n|    ep_rew_mean        | 94.4     |\n| time/                 |          |\n|    fps                | 2563     |\n|    iterations         | 9900     |\n|    time_elapsed       | 154      |\n|    total_timesteps    | 396000   |\n| train/                |          |\n|    entropy_loss       | -0.393   |\n|    explained_variance | 0.875    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9899     |\n|    policy_loss        | 0.642    |\n|    value_loss         | 4.44     |\n------------------------------------\n------------------------------------\n| eval/                 |          |\n|    mean_ep_length     | 260      |\n|    mean_reward        | 56.5     |\n| time/                 |          |\n|    total_timesteps    | 400000   |\n| train/                |          |\n|    entropy_loss       | -0.669   |\n|    explained_variance | 0.99     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9999     |\n|    policy_loss        | -0.0304  |\n|    value_loss         | 1.79     |\n------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 251      |\n|    ep_rew_mean     | 114      |\n| time/              |          |\n|    fps             | 2495     |\n|    iterations      | 10000    |\n|    time_elapsed    | 160      |\n|    total_timesteps | 400000   |\n---------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 241      |\n|    ep_rew_mean        | 125      |\n| time/                 |          |\n|    fps                | 2499     |\n|    iterations         | 10100    |\n|    time_elapsed       | 161      |\n|    total_timesteps    | 404000   |\n| train/                |          |\n|    entropy_loss       | -0.432   |\n|    explained_variance | 0.986    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10099    |\n|    policy_loss        | 0.0364   |\n|    value_loss         | 2.46     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 237      |\n|    ep_rew_mean        | 127      |\n| time/                 |          |\n|    fps                | 2503     |\n|    iterations         | 10200    |\n|    time_elapsed       | 162      |\n|    total_timesteps    | 408000   |\n| train/                |          |\n|    entropy_loss       | -0.308   |\n|    explained_variance | 0.992    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10199    |\n|    policy_loss        | 0.11     |\n|    value_loss         | 0.689    |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 235      |\n|    ep_rew_mean        | 129      |\n| time/                 |          |\n|    fps                | 2506     |\n|    iterations         | 10300    |\n|    time_elapsed       | 164      |\n|    total_timesteps    | 412000   |\n| train/                |          |\n|    entropy_loss       | -0.273   |\n|    explained_variance | 0.99     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10299    |\n|    policy_loss        | -0.00742 |\n|    value_loss         | 1.24     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 228      |\n|    ep_rew_mean        | 134      |\n| time/                 |          |\n|    fps                | 2510     |\n|    iterations         | 10400    |\n|    time_elapsed       | 165      |\n|    total_timesteps    | 416000   |\n| train/                |          |\n|    entropy_loss       | -0.456   |\n|    explained_variance | 0.997    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10399    |\n|    policy_loss        | -0.334   |\n|    value_loss         | 2.53     |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 233       |\n|    ep_rew_mean        | 143       |\n| time/                 |           |\n|    fps                | 2514      |\n|    iterations         | 10500     |\n|    time_elapsed       | 167       |\n|    total_timesteps    | 420000    |\n| train/                |           |\n|    entropy_loss       | -5.61e-10 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 10499     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.0528    |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 252      |\n|    ep_rew_mean        | 121      |\n| time/                 |          |\n|    fps                | 2520     |\n|    iterations         | 10600    |\n|    time_elapsed       | 168      |\n|    total_timesteps    | 424000   |\n| train/                |          |\n|    entropy_loss       | -1.7e-10 |\n|    explained_variance | 0.998    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10599    |\n|    policy_loss        | -0       |\n|    value_loss         | 0.0437   |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 270       |\n|    ep_rew_mean        | 96.2      |\n| time/                 |           |\n|    fps                | 2525      |\n|    iterations         | 10700     |\n|    time_elapsed       | 169       |\n|    total_timesteps    | 428000    |\n| train/                |           |\n|    entropy_loss       | -1.53e-06 |\n|    explained_variance | 0.994     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 10699     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.0329    |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 293       |\n|    ep_rew_mean        | 70.8      |\n| time/                 |           |\n|    fps                | 2530      |\n|    iterations         | 10800     |\n|    time_elapsed       | 170       |\n|    total_timesteps    | 432000    |\n| train/                |           |\n|    entropy_loss       | -1.38e-07 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 10799     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00117   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 315       |\n|    ep_rew_mean        | 46.6      |\n| time/                 |           |\n|    fps                | 2535      |\n|    iterations         | 10900     |\n|    time_elapsed       | 171       |\n|    total_timesteps    | 436000    |\n| train/                |           |\n|    entropy_loss       | -1.31e-05 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 10899     |\n|    policy_loss        | 3.47e-08  |\n|    value_loss         | 0.00829   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 337       |\n|    ep_rew_mean        | 23.7      |\n| time/                 |           |\n|    fps                | 2541      |\n|    iterations         | 11000     |\n|    time_elapsed       | 173       |\n|    total_timesteps    | 440000    |\n| train/                |           |\n|    entropy_loss       | -6.66e-06 |\n|    explained_variance | 0.995     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 10999     |\n|    policy_loss        | -4.23e-08 |\n|    value_loss         | 0.0119    |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 357      |\n|    ep_rew_mean        | 5.33     |\n| time/                 |          |\n|    fps                | 2546     |\n|    iterations         | 11100    |\n|    time_elapsed       | 174      |\n|    total_timesteps    | 444000   |\n| train/                |          |\n|    entropy_loss       | -6.4e-08 |\n|    explained_variance | 0.999    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11099    |\n|    policy_loss        | -0       |\n|    value_loss         | 0.000111 |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 380       |\n|    ep_rew_mean        | -15.7     |\n| time/                 |           |\n|    fps                | 2550      |\n|    iterations         | 11200     |\n|    time_elapsed       | 175       |\n|    total_timesteps    | 448000    |\n| train/                |           |\n|    entropy_loss       | -1.11e-06 |\n|    explained_variance | 0.995     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11199     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00651   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 403       |\n|    ep_rew_mean        | -38.4     |\n| time/                 |           |\n|    fps                | 2555      |\n|    iterations         | 11300     |\n|    time_elapsed       | 176       |\n|    total_timesteps    | 452000    |\n| train/                |           |\n|    entropy_loss       | -3.92e-08 |\n|    explained_variance | 0.999     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11299     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000316  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 424       |\n|    ep_rew_mean        | -59.8     |\n| time/                 |           |\n|    fps                | 2560      |\n|    iterations         | 11400     |\n|    time_elapsed       | 178       |\n|    total_timesteps    | 456000    |\n| train/                |           |\n|    entropy_loss       | -4.29e-08 |\n|    explained_variance | 0.997     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11399     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00297   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 447       |\n|    ep_rew_mean        | -84.7     |\n| time/                 |           |\n|    fps                | 2564      |\n|    iterations         | 11500     |\n|    time_elapsed       | 179       |\n|    total_timesteps    | 460000    |\n| train/                |           |\n|    entropy_loss       | -6.38e-09 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11499     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00133   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 470       |\n|    ep_rew_mean        | -107      |\n| time/                 |           |\n|    fps                | 2569      |\n|    iterations         | 11600     |\n|    time_elapsed       | 180       |\n|    total_timesteps    | 464000    |\n| train/                |           |\n|    entropy_loss       | -8.79e-07 |\n|    explained_variance | 0.992     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11599     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00227   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 490       |\n|    ep_rew_mean        | -138      |\n| time/                 |           |\n|    fps                | 2574      |\n|    iterations         | 11700     |\n|    time_elapsed       | 181       |\n|    total_timesteps    | 468000    |\n| train/                |           |\n|    entropy_loss       | -4.29e-08 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11699     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000908  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2578      |\n|    iterations         | 11800     |\n|    time_elapsed       | 183       |\n|    total_timesteps    | 472000    |\n| train/                |           |\n|    entropy_loss       | -4.98e-06 |\n|    explained_variance | 0.985     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11799     |\n|    policy_loss        | -9.27e-09 |\n|    value_loss         | 0.00258   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2583      |\n|    iterations         | 11900     |\n|    time_elapsed       | 184       |\n|    total_timesteps    | 476000    |\n| train/                |           |\n|    entropy_loss       | -5.96e-07 |\n|    explained_variance | 0.992     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11899     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.0024    |\n-------------------------------------\n-------------------------------------\n| eval/                 |           |\n|    mean_ep_length     | 500       |\n|    mean_reward        | -152      |\n| time/                 |           |\n|    total_timesteps    | 480000    |\n| train/                |           |\n|    entropy_loss       | -2.28e-08 |\n|    explained_variance | 0.992     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 11999     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00203   |\n-------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 500      |\n|    ep_rew_mean     | -152     |\n| time/              |          |\n|    fps             | 2480     |\n|    iterations      | 12000    |\n|    time_elapsed    | 193      |\n|    total_timesteps | 480000   |\n---------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2485      |\n|    iterations         | 12100     |\n|    time_elapsed       | 194       |\n|    total_timesteps    | 484000    |\n| train/                |           |\n|    entropy_loss       | -4.85e-07 |\n|    explained_variance | 0.988     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 12099     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00204   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2490      |\n|    iterations         | 12200     |\n|    time_elapsed       | 195       |\n|    total_timesteps    | 488000    |\n| train/                |           |\n|    entropy_loss       | -0.000537 |\n|    explained_variance | 0.99      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 12199     |\n|    policy_loss        | -3.31e-06 |\n|    value_loss         | 0.00208   |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -152      |\n| time/                 |           |\n|    fps                | 2495      |\n|    iterations         | 12300     |\n|    time_elapsed       | 197       |\n|    total_timesteps    | 492000    |\n| train/                |           |\n|    entropy_loss       | -4.77e-10 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 12299     |\n|    policy_loss        | -0        |\n|    value_loss         | 7.92e-05  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2499      |\n|    iterations         | 12400     |\n|    time_elapsed       | 198       |\n|    total_timesteps    | 496000    |\n| train/                |           |\n|    entropy_loss       | -3.03e-08 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 12399     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.000451  |\n-------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 500       |\n|    ep_rew_mean        | -151      |\n| time/                 |           |\n|    fps                | 2504      |\n|    iterations         | 12500     |\n|    time_elapsed       | 199       |\n|    total_timesteps    | 500000    |\n| train/                |           |\n|    entropy_loss       | -2.42e-10 |\n|    explained_variance | 0.998     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 12499     |\n|    policy_loss        | -0        |\n|    value_loss         | 0.00162   |\n-------------------------------------\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "✓ MODEL SAVED: ./models/a2c/a2c_baseline.zip\nReward: -151.2 ± 2.7 | Steps/Episode: 500.0\nSuccess: 0.0% | Convergence: 386784 steps\n",
          "output_type": "stream"
        },
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "({'config': 'baseline',\n  'mean_reward': -151.16,\n  'std_reward': 2.72,\n  'avg_steps_per_episode': 500.0,\n  'success_rate_%': 0.0,\n  'convergence_timestep': 386784,\n  'train_time_min': 3.33,\n  'total_timesteps': 500000,\n  'model_path': './models/a2c/a2c_baseline.zip'},\n <__main__.AdvancedMetricsCallback at 0x7e9890198890>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "all_results = []\n",
        "for f in os.listdir(f\"{base_dir}/results/a2c\"):\n",
        "    if f.endswith(\".json\"):\n",
        "        with open(f\"{base_dir}/results/a2c/{f}\") as j:\n",
        "            data = json.load(j)\n",
        "            all_results.append({\n",
        "                \"Config\": data[\"config\"],\n",
        "                \"Mean Reward\": data[\"mean_reward\"],\n",
        "                \"Steps/Episode\": data[\"avg_steps_per_episode\"],\n",
        "                \"Success Rate (%)\": data[\"success_rate_%\"],\n",
        "                \"Convergence (steps)\": data[\"convergence_timestep\"] or 500000,\n",
        "                \"Train Time (min)\": data[\"train_time_min\"],\n",
        "                \"Rewards\": data[\"rewards_history\"],\n",
        "                \"Lengths\": data[\"lengths_history\"]\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"Mean Reward\", ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "for i, res in enumerate(all_results):\n",
        "    axes[0,0].plot(np.convolve(res[\"Rewards\"], np.ones(20)/20, mode='valid'), label=res[\"Config\"], linewidth=2)\n",
        "axes[0,0].axhline(230, color='green', linestyle='--', label=\"Full Success\")\n",
        "axes[0,0].set_title(\"Average Reward Progress (Smoothed)\", fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlabel(\"Episodes\"); axes[0,0].set_ylabel(\"Reward\")\n",
        "axes[0,0].legend(); axes[0,0].grid(alpha=0.3)\n",
        "\n",
        "for i, res in enumerate(all_results):\n",
        "    axes[0,1].plot(np.convolve(res[\"Lengths\"], np.ones(20)/20, mode='valid'), label=res[\"Config\"], linewidth=2)\n",
        "axes[0,1].set_title(\"Steps per Episode (Efficiency - Lower Better)\", fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel(\"Episodes\"); axes[0,1].set_ylabel(\"Steps\")\n",
        "axes[0,1].legend(); axes[0,1].grid(alpha=0.3)\n",
        "\n",
        "df.plot(x=\"Config\", y=[\"Mean Reward\", \"Steps/Episode\"], kind=\"bar\", ax=axes[1,0], alpha=0.8)\n",
        "axes[1,0].set_title(\"Average Reward & Steps per Episode\", fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_ylabel(\"Score\"); axes[1,0].tick_params(rotation=0)\n",
        "\n",
        "df.plot(x=\"Config\", y=\"Convergence (steps)\", kind=\"barh\", ax=axes[1,1], color='teal', alpha=0.7, legend=False)\n",
        "axes[1,1].set_title(\"Convergence Time (Lower = Faster)\", fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_xlabel(\"Timesteps\")\n",
        "\n",
        "plt.suptitle(\"A2C Performance Analysis on MissionEnvironment\", fontsize=18, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"A2C_Performance_Graphs.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wO9jDSwpRjGk"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}